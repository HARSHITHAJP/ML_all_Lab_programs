# -*- coding: utf-8 -*-
"""2147250_all_lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yMmROPdTF-726v4HhIv6tva_LF_IWZLS

**DATASET DESCRIPTION**


**Dataset name : Austin Weather**

Dataset description :
The dataset is derived from Kaggle’s repository.
The forecasts are generated from Austin KATT station’s proprietary forecasting system that leverages their vast amount of neighborhood weather data that they get from their community.
 
Dataset Attributes:
The dataset has 21 attributes. Each tells a measure used to display as the weather forecast
Attributes
Description
Date
(YYYY-MM-DD)  
Dates are from 2013-12-21 to 2017-07-31
<br>
TempHighF
 (High temperature, in Fahrenheit)
 <br>
TempAvgF
(Average temperature, in Fahrenheit)
<br>
TempLowF
(Low temperature, in Fahrenheit)
<br>
DewPointHighF
(High dew point, in Fahrenheit)
<br>
DewPointAvgF
 (Average dew point, in Fahrenheit)
 <br>
DewPointLowF
 (Low dew point, in Fahrenheit)
 <br>
HumidityHighPercent
 (High humidity, as a percentage)
 <br>
HumidityAvgPercent 
 (Average humidity, as a percentage)
 <br>
HumidityLowPercent 
(Low humidity, as a percentage)
<br>
SeaLevelPressureHighInches
(High sea level pressure, in inches)
<br>
SeaLevelPressureAvgInches
(Average sea level pressure, in inches)
<br>
SeaLevelPressureLowInches
(Low sea level pressure, in inches)
<br>
VisibilityHighMiles
 (High visibility, in miles)
 <br>
VisibilityAvgMiles
(Average visibility, in miles)
<br>
VisibilityLowMiles
 (Low visibility, in miles)
 <br>
WindHighMPH
 (High wind speed, in miles per hour)
 <br>
WindAvgMPH
(Average wind speed, in miles per hour)
<br>
WindGustMPH
 (Highest wind speed gust, in miles per hour)
 <br>
PrecipitationSumInches
Total precipitation, in inches) ('T' if Trace)
<br>
Events
(Adverse weather events. ' ' if None)(Nan)
<br>

class attribute:
'Events'

Dataset Sample:
Dataset has 1320 samples collected by Austin KATT station.
"""

from google.colab import files
 
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import io
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
# %matplotlib inline
import warnings
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

warnings.filterwarnings('ignore')

df = pd.read_csv(io.BytesIO(uploaded['austin_weather_preprocessed_2147250.csv']))
print(df.head().to_markdown())

df.columns

#Drop the columns with the non numerical values.
df1 = df
num_df = df1.drop(['Date'], axis=1)

num_df.columns
num_df.hist()
num_df['Events']=labelencoder.fit_transform(num_df['Events'])
# Bring the class attribute as the last column

temp_series = num_df.pop('Events')
num_df['Events'] = temp_series
num_df.columns

df.head()

sns.heatmap(df.isnull())

"""It is a heat map which is used here to depict null values in different color.
If there are no null values then whole heatmap would be of one color.
but here in 'Events' attribute there are few null values.
"""

df.fillna(0, inplace = True)
sns.heatmap(df.isnull())

"""There are no null values hence whole heatmap is depicted in one color."""

df.nunique()

num_df.columns
num_df.hist()

"""We have plotted the histograms for all the relevant independent features along with the dependant feature.<br>
This can be used to see the quantities or number of items for each feature.
<br> example - IN 'Events' attribute more samples belong to EVENT 7
"""

df['Year'] = pd.DatetimeIndex(df['Date']).year
df.head()

"""**lab1 ---------------------------------------------------------------EDA-----------------------------------------------------------------------------------------**"""

import matplotlib.pyplot as plt
grp_tempavg_yr=df['TempAvgF'].groupby(df['Year']).mean()

"""line plot"""

grp_tempavg_yr.plot(x="TempAvgF", y="Year")

"""we can see that around 2013 to 14  thre is steep increase in AVGtempF but in after 2014 to 2017 there is gradual increase

**Box plot** - display the summary of the set of data values having properties like minimum, first quartile, median, third quartile and maximum.
"""

boxplot = sns.boxplot(x='Year', y='TempHighF',  data=df)

"""**Map func** - map() function returns a map object(which is an iterator) of the results after applying the given function to each item of a given iterable (list, tuple etc.)

<br>
We can infer that there are quite few outliers in each years data set.
"""

mapfunc = df['TempAvgF'].map(lambda x:x>50)
print(mapfunc)

"""**Rename** - rename() method is used to rename any index, column or row. Renaming of column can also be done by dataframe."""

df.rename(str.lower, axis='columns')

"""**Bar graph** -  A bar chart describes the comparisons between the discrete categories."""

bargraph=grp_tempavg_yr.plot.bar(x="Year", y="TempAvgF")

"""This bar graph helps us to identify the quantity of data samples belonging to each year agains the average temperature"""

barhgraph=grp_tempavg_yr.plot.barh(x="Year", y="TempAvgF")

"""This bar graph helps us to identify the quantity of data samples belonging to each year agains the average temperature"""

histplot = grp_tempavg_yr.plot.hist( by="Year")

"""**Pair plot**- Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical.


"""

col=['TempAvgF', 'TempHighF', 'Year']
sns.pairplot(df[col],  hue ='Year')
plt.show()

"""**Pair plot**- Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical.

"""

sns.distplot(df.TempAvgF)

"""**Dist plot** - It represents the data distribution of a variable against the density distribution. """

sns.jointplot(x = "Year", y = "TempAvgF",kind = "hex", data = df)

"""Joint plot - helps to explore relationships between bivariate data, as well as their distributions at the same time"""

sns.stripplot(x="Year", y="TempAvgF", data=df)

"""Stripplot is helping us to clearly depict the quantity of samples belonging to each year."""

sns.violinplot(x="Year", y="TempAvgF", data=df)

hm = sns.heatmap(df.corr())

"""Heat map depicting the correlation between each features and hence we can identify the most important features."""

sns.scatterplot(data=df, x="TempAvgF", y="HumidityAvgPercent", )

"""scatter plot of humidity vs average temperature for the given samples."""

plot = grp_tempavg_yr.plot.pie(y='TempAvgF', autopct='%1.0f%%', figsize=(5, 5))

"""Average temperature in PIE graph depicting that most of the samples have avearge temperature."""

df.corr()

from sklearn.impute import SimpleImputer
mean_imputer = SimpleImputer(missing_values=0, strategy='mean', fill_value=None)
print(df)

"""--------------------------------------------- list for all accuracy -----------------------------------------"""

classiferList = ["KNN", "Naive bayes", "K means", "Hierarchial", "Decision Tree", "Linear Regression", "Logistic regression", "SVM", "MLP"]
accuracyList = []

acc_dict = {'KNN':0, "Naive bayes":0, "Kmeans":0, "Hierarchial":0, "DecisionTree":0, "LinearRegression":0, "LogisticRegression":0, "SVM":0,"MLP":0}

"""**lab2 ---------------------------------------------------------------KNN-----------------------------------------------------------------------------------------**

K-NN algorithm can be used for Classification as well as for Regression.
It is used for Supervised Learning.
<br>

It classifies the data point on how its neighbor is classified
"""

from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler 
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)

#here we have given k values from 2 to 6 and n= 1, 2, 3
for i in range(2,7):
  for n in range(1,4):
    print("----------------------------------------K = "+str(i)+" N ="+str(n))
    classifier= KNeighborsClassifier(n_neighbors=5, p=1)  
    classifier.fit(x_train, y_train) 
    y_pred = classifier.predict(x_test)


    result = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(result)
    result1 = classification_report(y_test, y_pred)
    print("Classification Report:",)
    print (result1)
    result2 = accuracy_score(y_test,y_pred)
    print("Accuracy:",result2*100)

classifier = KNeighborsClassifier(n_neighbors=5,p=2)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test,y_pred))
acc_score = accuracy_score(y_pred,y_test) 
print("Accuracy score",acc_score)
acc_dict['KNN']=round(acc_score,3)
print(acc_dict)

"""**lab3 --------------------------------------------------------------Naive Bayes-------------------------------------------------------------------------------**

It is a probabilistic classifier for supervised learning which means it predicts on the basis of the probability of an object.
"""

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)  


# train a Gaussian Naive Bayes classifier on the training set
from sklearn.naive_bayes import GaussianNB

# instantiate the model
gnb = GaussianNB()

# fit the model
gnb.fit(x_train, y_train)

# predicting the values
y_pred = gnb.predict(x_test)
y_pred

y_pred_train = gnb.predict(x_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

# print the scores on training and test set to check over fitting and underfitting

print('Training set score: {:.4f}'.format(gnb.score(x_train, y_train)))
print('Test set score: {:.4f}'.format(gnb.score(x_test, y_test)))

from pandas.core.dtypes.cast import maybe_box_native
# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)

max_acc = max_k = max_n = 0
for k in range(2,7):
  for n in range(1,4):
    classifier = KNeighborsClassifier(n_neighbors=k,p=n)
    classifier.fit(x_train, y_train)
    y_pred = classifier.predict(x_test)
    accuracy = metrics.accuracy_score(y_test, y_pred)

    if accuracy > max_acc:
      max_acc = accuracy
      max_k = k
      max_n = n

if max_n == 1:
  distance_metric = "manhattan"
elif max_n == 2:
  distance_metric = "euclidean"
elif max_n == 3:
  distance_metric = "minkowsky"


print("--------------------------MAX ACCURACY AMONG KNN CLASSIFIER--------------------------------- ")
print("Max accuracy                             : ",max_acc*100)
print("Distance Metric                          : ",distance_metric)
print("N Distance measurement                   : ",max_n)
print("K number of neighbours for max accuracy  : ",max_k)

acc_score = accuracy_score(y_pred,y_test) 
print("Accuracy score",acc_score)
acc_dict["Naive bayes"] =round(acc_score,3)
print(acc_dict)

"""**lab4 ----------------------------------------------------------K Means clustering -----------------------------------------------------------------------**

K-Means Clustering is an Unsupervised Learning algorithm, which groups the unlabeled dataset into different clusters. Here K defines the number of pre-defined clusters that need to be created in the process, as if K=2, there will be two clusters, and for K=3, there will be three clusters, and so on.
"""

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)  

#finding optimal number of clusters using the elbow method  
from sklearn.cluster import KMeans  
wcss_list= []  #Initializing the list for the values of WCSS  
  
#Using for loop for iterations from 1 to 6.  
for i in range(1, 7):  
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)  
    kmeans.fit(x)  
    wcss_list.append(kmeans.inertia_)  
plt.plot(range(1, 7), wcss_list)  
plt.title('The Elobw Method Graph')  
plt.xlabel('Number of clusters(k)')  
plt.ylabel('wcss_list')  
plt.show()

"""We have used lineplot to depict the best clusters by setting different number of cluster to the naive bayes classifier, hence finding the optimal clusters using the WCSS value obtained during each classification. <br>
From this graph we can infer that the optimal cluster value is 3 as the slope becomes normal
"""

#training the K-means model on a dataset  
kmeans = KMeans(n_clusters=6, init='k-means++', random_state= 42)  
y_predict= kmeans.fit_predict(x)

y_predict

kmeans = KMeans(n_clusters=5, random_state=0) 

kmeans.fit(x)
y_pred = kmeans.predict(x)

kmeans.cluster_centers_
kmeans.inertia_

labels = kmeans.labels_
correct_labels = sum(y == labels)
acc_score = correct_labels/float(y.size)
print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
print('Accuracy score: {0:0.2f}'. format(acc_score))
acc_dict['Kmeans']=round(acc_score,3)
print(acc_dict)

"""**lab5 -----------------------------------------------------------Hierarchial clustering -----------------------------------------------------------------------**

Hierarchical clustering is another unsupervised machine learning algorithm, which is used to group the unlabeled datasets into a cluster and also known as hierarchical cluster analysis
"""

X= num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

import scipy.cluster.hierarchy as sch
dendogram = sch.dendrogram(sch.linkage(X, method= 'ward', metric= 'euclidean'))

"""We can clearly visualize the steps of hierarchical clustering. More the distance of the vertical lines in the dendrogram, more the distance between those clusters.

Now, we can set a threshold distance and draw a horizontal line (Generally, we try to set the threshold in such a way that it cuts the tallest vertical line).
<br> we can see that there are 2 prominent clusters forming.
"""

from sklearn.cluster import AgglomerativeClustering  
hc= AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
X_D = hc.fit(X)
y_pred= hc.fit_predict(X)  
# print the class labels
print(hc.labels_)

acc_score = accuracy_score(y,y_pred)
acc_dict['Hierarchial']=round(acc_score,3)
print(acc_dict)

plt.figure(figsize=(10, 7))  
plt.scatter(num_df['TempHighF'], num_df['Events']) 
plt.xlabel("TempHighF")
plt.ylabel("Events")

"""Scatter plot for a categorical dataset hence we have got the samples in horizontal lines.

**K means VS Hierarchical**

k means
you should have prior knowledge about the k


---


each data will belong exactly one subset


---


**hierarchical**
no need of k values


---


It is a set of nested clusters that are arranged as a tree.

**lab6 ----------------------------------------------Decision Tree & Linear Regression-------------------------------------------------------------------**

It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.
"""

# importing the sklearn library
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler 
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.tree import DecisionTreeClassifier

# getting the x(dataset excluding the class attribute) and y(class attribute) dataset from class data set
x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

#training set size 0.75 - x_train + y_train
#test set size 0.25 - x_test + y_test
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)


#reduce the mean to ‘0’ and the standard deviation to ‘1’.
st_x= StandardScaler()    
x_train= st_x.fit_transform(x_train)    
x_test= st_x.transform(x_test)


#Gini = 0 means it is the purest node and it is a leaf.
#A Gini coefficient of 0 expresses perfect equality, where all values are the same, while a Gini coefficient of 1 (or 100%) expresses maximal inequality among values. For example, if everyone has the same income, the Gini coefficient will be 0.
clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)

# fit the model
clf_gini.fit(x_train, y_train)

#x_test acts as question

y_pred_gini = clf_gini.predict(x_test)
print('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))

#answer and predicted
print(classification_report(y_test, y_pred_gini))

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf_gini, x_test, y_test)

"""This heat map is applied for the heat map and it depicts the correctly and wrongly prdicted number of samples. Most of the smaples belonging to class 8 are correctly predicted where others are not predicted. Also there are more flase postives in class 4."""

y_pred_train_gini = clf_gini.predict(x_train)

y_pred_train_gini
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))


print(classification_report(y_train, y_pred_train_gini))

print(confusion_matrix(y_train, y_pred_train_gini))

print('Training set score: {:.4f}'.format(clf_gini.score(x_train, y_train)))

print('Test set score: {:.4f}'.format(clf_gini.score(x_test, y_test)))

plt.figure(figsize=(12,8))

from sklearn import tree

tree.plot_tree(clf_gini.fit(x_train, y_train)) 

#use of dt----for feature engineering---- to select the best column

"""We can see from the tree that 15th column is taken as root node hence the class attribute has more dependency on the 15th column."""

acc_score = accuracy_score(y_pred_gini,y_test) 
print("Accuracy score",acc_score)
acc_dict['DecisionTree']=round(acc_score,3)
print(acc_dict)

"""linear regression
<br>
It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables
"""

from sklearn import linear_model

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)  
lm = linear_model.LinearRegression()
model = lm.fit(x_train,y_train)
pred = lm.predict(x_train)

#checking accuracy:
from sklearn.metrics import r2_score
predd = lm.predict(x_test)
acc_score = r2_score(y_test, predd)
acc_dict['LinearRegression']=round(acc_score,3)
print(acc_dict)

"""**lab7 ------------------------------------------------------- Logistic Regression -----------------------------------------------------------------------**"""

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)  

#Fitting Logistic Regression to the training set  
from sklearn.linear_model import LogisticRegression  
classifier= LogisticRegression(random_state=0)  
classifier.fit(x_train, y_train)

#Predicting the test set result  
y_pred= classifier.predict(x_test)  

#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix  
cm= confusion_matrix(y_test, y_pred)  
cm

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(classifier, x_test, y_test)

"""This heat map is applied for the heat map and it depicts the correctly and wrongly prdicted number of samples. Most of the smaples belonging to class 8 are correctly predicted where others are not predicted. Also there are more flase postives in class 4."""

print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(x_test, y_test)))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

tested_x = classifier.predict(x_test)

from sklearn import metrics
log_regression = LogisticRegression()

log_regression.fit(x_train,y_train)
#define metrics
y_pred_proba = log_regression.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba, pos_label=0)

#create ROC curve
plt.plot(fpr,tpr ,color="navy")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""The graph shows the ROC curve and since the curve is left dominant, we can see that the classifer is able to classify correctly."""

acc_score = accuracy_score(y_test, y_pred) 
print("Accuracy score",acc_score)
acc_dict['LogisticRegression']=round(acc_score,3)
print(acc_dict)

"""**lab8 -----------------------------------------------------SVM -----------------------------------------------------------------------**"""

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)  

from sklearn import svm
#create a classifier
svm_cls = svm.SVC(kernel="linear")
#train the model
svm_cls.fit(x_train,y_train)
#predict the response
pred = svm_cls.predict(x_test)

from sklearn import metrics
#accuracy
y_pred=pred
print("acuracy:", metrics.accuracy_score(y_test,y_pred=pred))
print("Precision Score : ",  metrics.precision_score(y_test,y_pred,pos_label='positive', average='micro'))
print("Recall Score :" ,  metrics.recall_score(y_test, y_pred, pos_label='positive', average='micro') )
print(metrics.classification_report(y_test, y_pred=pred))

#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix  
cm= confusion_matrix(y_test, y_pred)  
cm

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(svm_cls, x_test, y_test)

"""This heat map is applied for the heat map and it depicts the correctly and wrongly prdicted number of samples. Most of the smaples belonging to class 8 are correctly predicted where others are not predicted. Also there are more flase postives in class 4."""

acc_score = accuracy_score(y_test, y_pred) 
print("Accuracy score",acc_score)
acc_dict['SVM']=round(acc_score,3)
print(acc_dict)

"""**lab9 -------------------------------------------------MLP-----------------------------------------------------------------------**"""

x = num_df.drop('Events',axis=1) #independant variables
y = num_df['Events'] #dependant

x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25)  
sc=StandardScaler()

scaler = sc.fit(x_train)
trainX_scaled = scaler.transform(x_train)
testX_scaled = scaler.transform(x_test)

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(random_state=1, max_iter=300).fit(x_train, y_train)

clf.predict_proba(x_test)
clf.score(x_test, y_test)

pred = clf.score(x_train,y_train)
pred

activationList = ["relu", "identity", "logistic", "tanh"]
for i in range(0,4):
  clf = MLPClassifier(activation = activationList[i]);
  clf.fit(x_train, y_train);
  tempscore = clf.score(x_train, y_train)
  print("Activation function -",activationList[i],"- Accuracy : ",tempscore)

y_pred = clf.predict(testX_scaled)

print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

#Get the confusion matrix
cf_matrix = confusion_matrix(y_test, y_pred)
print(cf_matrix)

sns.heatmap(cf_matrix, annot=True)

"""This heat map is applied for the heat map and it depicts the correctly and wrongly prdicted number of samples. Most of the smaples belonging to class 7 are correctly predicted where others are not predicted."""

acc_score = accuracy_score(y_test, y_pred) 
print("Accuracy score",acc_score)
acc_dict['MLP']=round(acc_score,3)
print(acc_dict)

"""**lab10 -------------------------------------------BPN---------------------------------------------------------------------**"""

from sklearn.metrics import roc_auc_score

# getting the x(dataset excluding the class attribute) and y(class attribute) dataset from class data set
X = num_df.drop('Events',axis=1)
y = num_df['Events']

X=X.values
y=y.values
y = pd.get_dummies(y).values

#Split data into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=4)

clf = MLPClassifier(hidden_layer_sizes=(100,),random_state=1, max_iter=300).fit(X_train, y_train)
clf.fit(X_train,y_train)
y_pred = clf.predict_proba(X_train)
print('Overall AUC:', roc_auc_score(y_train, clf.predict_proba(X_train),multi_class="ovr"))

# Initialize variables
learning_rate = 0.1
iterations = 500
N = y_train.size

# number of input features
input_size = 19 #number of other columns(independent)

# number of hidden layers neurons
hidden_size = 2 

# number of neurons at the output layer
output_size = 9 #number of classesin class attribute  

results = pd.DataFrame(columns=["mse", "accuracy"])

# Initialize weights
np.random.seed(10)

# initializing weight for the hidden layer
W1 = np.random.normal(scale=0.5, size=(input_size, hidden_size))   

# initializing weight for the output layer
W2 = np.random.normal(scale=0.5, size=(hidden_size , output_size))

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def mean_squared_error(y_pred, y_true):
    return ((y_pred - y_true)**2).sum() / (2*y_pred.size)
    
def accuracy(y_pred, y_true):
    acc = y_pred.argmax(axis=1) == y_true.argmax(axis=1)
    return acc.mean()

for itr in range(iterations):    
    
    # feedforward propagation
    # on hidden layer
    Z1 = np.dot(X_train, W1)
    A1 = sigmoid(Z1)

    # on output layer
    Z2 = np.dot(A1, W2)
    A2 = sigmoid(Z2)
    
    
    # Calculating error
    mse = mean_squared_error(A2, y_train)
    acc = accuracy(A2, y_train)
    results=results.append({"mse":mse, "accuracy":acc},ignore_index=True )
    
    # backpropagation
    E1 = A2 - y_train
    dW1 = E1 * A2 * (1 - A2)

    E2 = np.dot(dW1, W2.T)
    dW2 = E2 * A1 * (1 - A1)

    
    # weight updates
    W2_update = np.dot(A1.T, dW1) / N
    W1_update = np.dot(X_train.T, dW2) / N

    W2 = W2 - learning_rate * W2_update
    W1 = W1 - learning_rate * W1_update

results.mse.plot(title="Mean Squared Error")

"""The error decreses with each iteraton. the grapgh clearly depicts number of iterations vs error rates."""

results.accuracy.plot(title="Accuracy")

"""We can see the accuracy increases after the 200 iterations

"""

y_pred = clf.predict_proba(X_train)
print('Overall AUC:', roc_auc_score(y_train,y_pred,multi_class="ovr"))

"""**all accuracy scores** """

keyList = []
valuesList = []

for key, value in acc_dict.items():
  keyList.append(key)
  valuesList.append(value)

plt.figure(figsize=(20, 7))
plt.title("Algorithms vs accuracy scores")
plt.plot(keyList, valuesList)
plt.xlabel("ALGORITHMS")
plt.ylabel("ACCURACY SCORES")  

print(acc_dict)
print("\n ")

"""When KNN, Naive Bayes, K means,  Hierarchial, Decision tree, Linear regression, Logistic regression, SVM and MLP algorithms were compared, the logistic regression has given more accuracy than all the algorithms. hence it is the best algorithm for the classification of austin weather dataset."""